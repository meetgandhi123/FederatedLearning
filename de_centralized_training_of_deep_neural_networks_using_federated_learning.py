# -*- coding: utf-8 -*-
"""De-Centralized Training of Deep Neural Networks using Federated Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xuDQBH-QmbSJgeQDOwG664V3NBZYQWzK

# Abstract: 

There has been a sudden increase in the use of Deep Learning in tasks ranging from image classification to generating complex models like GPT 3. One of the most common things with the Machine Learning or Deep Learning pipeline is that the entire process is done on the same server. The training, testing, and validation data are on the same machine. In this post, we are going to talk about how decentralized training of deep learning models takes place and what are the advantages associated with this approach. We are using the TensorFlow-Federated framework along with the MNIST dataset to show the working of Federated Learning. We also analyze the model accuracy and processing time for each iteration with the number of clients present in the system.

# Loading Libraries
"""

!pip install --quiet --upgrade tensorflow-federated==0.20.0
!pip install --quiet --upgrade nest-asyncio

import nest_asyncio
nest_asyncio.apply()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

import collections
import numpy as np
import tensorflow as tf
import tensorflow_federated as tff

np.random.seed(0)

tff.federated_computation(lambda: 'Hello, World!')()

"""# Load Data"""

emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()
print(len(emnist_train.client_ids))
print(emnist_train.element_type_structure)

"""# Visualization of Data"""

example_dataset = emnist_train.create_tf_dataset_for_client(
    emnist_train.client_ids[0])

example_element = next(iter(example_dataset))

example_element['label'].numpy()

from matplotlib import pyplot as plt
plt.imshow(example_element['pixels'].numpy(), cmap='gray', aspect='equal')
plt.grid(False)
_ = plt.show()

# Number of examples per layer for a sample of clients
f = plt.figure(figsize=(12, 7))
f.suptitle('Label Counts for a Sample of Clients')
for i in range(6):
  client_dataset = emnist_train.create_tf_dataset_for_client(
      emnist_train.client_ids[i])
  plot_data = collections.defaultdict(list)
  for example in client_dataset:
    # Append counts individually per label to make plots
    # more colorful instead of one color per plot.
    label = example['label'].numpy()
    plot_data[label].append(label)
  plt.subplot(2, 3, i+1)
  plt.title('Client {}'.format(i))
  for j in range(10):
    plt.hist(
        plot_data[j],
        density=False,
        bins=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

"""# Pre-Process"""

NUM_CLIENTS = 5
NUM_EPOCHS = 200
BATCH_SIZE = 20
SHUFFLE_BUFFER = 100
PREFETCH_BUFFER = 10

def preprocess(dataset):

  def batch_format_fn(element):
    return collections.OrderedDict(
        x=tf.reshape(element['pixels'], [-1, 784]),
        y=tf.reshape(element['label'], [-1, 1]))

  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(
      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)

preprocessed_example_dataset = preprocess(example_dataset)

sample_batch = tf.nest.map_structure(lambda x: x.numpy(),
                                     next(iter(preprocessed_example_dataset)))

sample_batch

def make_federated_data(client_data, client_ids):
  return [
      preprocess(client_data.create_tf_dataset_for_client(x))
      for x in client_ids
  ]

sample_clients = emnist_train.client_ids[0:NUM_CLIENTS]

federated_train_data = make_federated_data(emnist_train, sample_clients)

print(f'Number of client datasets: {len(federated_train_data)}')
print(f'First dataset: {federated_train_data[0]}')

"""# model with Keras"""

from tensorflow import keras
from tensorflow.keras import layers

def create_keras_model():
  model = keras.Sequential(
    [
        keras.Input(shape=(784,)),
        layers.Dense(512, activation='relu'),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(10, activation="softmax"),
    ]
)

  return model

def model_fn():
  # We _must_ create a new model here, and _not_ capture it from an external
  # scope. TFF will call this within different graph contexts.
  keras_model = create_keras_model()
  return tff.learning.from_keras_model(
      keras_model,
      input_spec=preprocessed_example_dataset.element_spec,
      loss=tf.keras.losses.SparseCategoricalCrossentropy(),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

from keras.utils.vis_utils import plot_model
keras_model = create_keras_model()
print(keras_model.summary())
plot_model(keras_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

"""# Training the model on federated data"""

iterative_process = tff.learning.build_federated_averaging_process(
    model_fn,
    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),
    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))

state = iterative_process.initialize()

NUM_ROUNDS = 200
train_accuracy = []
test_accuracy = []
train_loss=[]
test_loss=[]

for round_num in range(1, NUM_ROUNDS):
  state, metrics = iterative_process.next(state, federated_train_data)

  tuple_list = list(metrics.items())
  temp = tuple_list[2][1]
  temp1= list(temp.items())
  train_accuracy.append(temp1[0][1])
  train_loss.append(temp1[1][1])

  print('round {:2d}, metrics={}'.format(round_num, metrics))

train_accuracy_plot = train_accuracy
test_accuracy_plot = test_accuracy

epochs = range(1,200)

plt.plot(epochs, train_accuracy_plot, 'b', label='Training Accuracy')
#plt.plot(epochs, test_accuracy_plot, 'b', label='Testing Accuracy')
plt.title('Training Accuracy vs Epoch')
plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.legend()
plt.show()

train_loss_plot = train_loss
test_loss_plot = test_loss

epochs = range(1,200)

plt.plot(epochs, train_loss_plot, 'b', label='Training Loss')
#plt.plot(epochs, test_loss_plot, 'b', label='Testing Loss')
plt.title('Training Loss vs Epoch')
plt.xlabel('Epochs')
plt.ylabel('Training Loss')
plt.legend()
plt.show()

"""# Eval"""

evaluation = tff.learning.build_federated_evaluation(model_fn)

train_metrics = evaluation(state.model, federated_train_data)

str(train_metrics)

federated_test_data = make_federated_data(emnist_test, sample_clients)

len(federated_test_data), federated_test_data[0]

test_metrics = evaluation(state.model, federated_test_data)

str(test_metrics)



"""# experimentations

"""

#NUM_CLIENTS = 4
client_list_custom=[2,4,5,6,8,10]

for cli_list in client_list_custom:
  print("Number of clients: ",cli_list)
  NUM_CLIENTS=cli_list  
  emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()
  example_dataset = emnist_train.create_tf_dataset_for_client(
      emnist_train.client_ids[0])

  example_element = next(iter(example_dataset))


  NUM_EPOCHS = 26
  BATCH_SIZE = 20
  SHUFFLE_BUFFER = 100
  PREFETCH_BUFFER = 10

  def preprocess(dataset):

    def batch_format_fn(element):
      return collections.OrderedDict(
          x=tf.reshape(element['pixels'], [-1, 784]),
          y=tf.reshape(element['label'], [-1, 1]))

    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(
        BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)

  preprocessed_example_dataset = preprocess(example_dataset)

  sample_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_example_dataset)))

  def make_federated_data(client_data, client_ids):
    return [
        preprocess(client_data.create_tf_dataset_for_client(x))
        for x in client_ids
    ]


  sample_clients = emnist_train.client_ids[0:NUM_CLIENTS]

  federated_train_data = make_federated_data(emnist_train, sample_clients)


  from tensorflow import keras
  from tensorflow.keras import layers

  def create_keras_model():
    model = keras.Sequential(
      [
          keras.Input(shape=(784,)),
          layers.Dense(512, activation='relu'),
          layers.Dense(256, activation='relu'),
          layers.Dropout(0.5),
          layers.Dense(10, activation="softmax"),
      ]
  )

    return model


  def model_fn():
    keras_model = create_keras_model()
    return tff.learning.from_keras_model(
        keras_model,
        input_spec=preprocessed_example_dataset.element_spec,
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])  
    
  from keras.utils.vis_utils import plot_model
  keras_model = create_keras_model()

  iterative_process = tff.learning.build_federated_averaging_process(
      model_fn,
      client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),
      server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))

  state = iterative_process.initialize()

  train_accuracy = []
  test_accuracy = []
  train_loss=[]
  test_loss=[]
  import time

  start = time.time()
  for round_num in range(1, NUM_EPOCHS):
    state, metrics = iterative_process.next(state, federated_train_data)

    tuple_list = list(metrics.items())
    temp = tuple_list[2][1]
    temp1= list(temp.items())
    train_accuracy.append(temp1[0][1])
    train_loss.append(temp1[1][1])
    if(round_num%5==0):
      print('--->round {:2d}, metrics={}'.format(round_num, metrics))
      if(cli_list==5 or cli_list==10):
        evaluation = tff.learning.build_federated_evaluation(model_fn)
        federated_test_data = make_federated_data(emnist_test, sample_clients)
        test_metrics = evaluation(state.model, federated_test_data)
        print("Testing accuracy")
        print(str(test_metrics))

  end=time.time()
  print("avg time for 1 epochs:",str((end-start)/25))

  evaluation = tff.learning.build_federated_evaluation(model_fn)
  federated_test_data = make_federated_data(emnist_test, sample_clients)
  test_metrics = evaluation(state.model, federated_test_data)
  print("Testing accuracy")
  print(str(test_metrics))
  print("-------------------------------------")

# evaluation = tff.learning.build_federated_evaluation(model_fn)
# federated_test_data = make_federated_data(emnist_test, sample_clients)
# test_metrics = evaluation(state.model, federated_test_data)
# str(test_metrics)

"""# References
1. Jakub Konečný and H. Brendan McMahan and Felix X. Yu and Peter Richtarik and Ananda Theertha Suresh and Dave Bacon. 2016. Federated Learning: Strategies for Improving Communication Efficiency. NIPS Workshop on Private Multi-Party Machine Learning. https://doi.org/10.48550/arxiv.1610.05492
2. Kai Hu, Yaogen Li, Min Xia, Jiasheng Wu, Meixia Lu, Shuai Zhang, Liguo Weng, "Federated Learning: A Distributed Shared Machine Learning Method", Complexity, vol. 2021, Article ID 8261663, 20 pages, 2021. https://doi.org/10.1155/2021/8261663
3. Qinbin Li and Zeyi Wen and Zhaomin Wu and Sixu Hu and Naibo Wang and Yuan Li and Xu Liu and Bingsheng He, "A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection", IEEE Transactions on Knowledge and Data Engineering, https://doi.org/10.1109%2Ftkde.2021.3124599
4. McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Agüera y, "Communication-Efficient Learning of Deep Networks from Decentralized Data", https://doi.org/10.48550/arxiv.1602.05629
5. https://flower.dev/docs/
6. https://github.com/OpenMined/PySyft
7. https://www.tensorflow.org/federated
8. Brendan McMahan and Daniel Ramage, Research Scientists Google AI Blog, "Federated Learning: Collaborative Machine Learning without Centralized Training Data" https://ai.googleblog.com/2017/04/federated-learning-collaborative.html.



"""